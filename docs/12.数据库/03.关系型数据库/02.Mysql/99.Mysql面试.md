---
icon: logos:mysql
title: Mysql 面试
date: 2020-09-12 10:43:53
order: 99
categories:
  - 数据库
  - 关系型数据库
  - Mysql
tags:
  - 数据库
  - 关系型数据库
  - Mysql
  - 面试
---

# Mysql 面试

## 架构

### 一条 SQL 查询语句是如何执行的？

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202310080719676.png)

1. **连接器**：连接器负责跟客户端建立连接、获取权限、维持和管理连接。
2. **查询缓存**：命中缓存，则直接返回结果。弊大于利，因为失效非常频繁——任何更新都会清空查询缓存。
3. **分析器**
   - **词法分析**：解析 SQL 关键字
   - **语法分析**：生成一颗对应的语法解析树
4. **优化器**
   - 根据语法树**生成多种执行计划**
   - **索引选择**：根据策略选择最优方式
5. **执行器**
   - 校验读写权限
   - 根据执行计划，调用存储引擎的 API 来执行查询
6. **存储引擎**：存储数据，提供读写接口

### 一条 SQL 更新语句是如何执行的？

更新流程和查询的流程大致相同，不同之处在于：更新流程还涉及两个重要的日志模块：

- redo log（重做日志）
  - InnoDB 存储引擎独有的日志（物理日志）
  - 采用循环写入
- binlog（归档日志）
  - Mysql Server 层通用日志（逻辑日志）
  - 采用追加写入

为了保证 redo log 和 binlog 的数据一致性，所以采用两阶段提交方式更新日志。

### 为什么表数据删掉一半，表文件大小不变

表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：

1. 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；
2. 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。

我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。

要删掉 R4 这个记录，InnoDB 引擎只会把 R4 这个记录标记为删除。如果之后要再插入一个 ID 在 300 和 600 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。

如果删掉了一个数据页上的所有记录，则整个数据页就可以被复用了。

如果把整个表的数据删除，则所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。

delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。

如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。页分裂完成后，就可能产生空洞。另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。

也就是说，经过大量增删改的表，都是可能是存在空洞的。

#### 重建表

那么，如何收缩表空间，去除空洞呢？

可以使用 `alter table A engine=InnoDB` 命令来重建表。MySQL 会自动完成转存数据、交换表名、删除旧表的操作。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726203135.png)

显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。

在**MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。**

1. 建立一个临时文件，扫描表 A 主键的所有数据页；
2. 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；
3. 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；
4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；
5. 用临时文件替换表 A 的数据文件。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726203250.png)

对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。

需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用 GitHub 开源的 gh-ost 来做。

optimize table、analyze table 和 alter table 这三种方式重建表的区别：

- 从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是上面图 4 的流程了；
- analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；
- optimize table t 等于 recreate+analyze。

### 为什么我的 MySQL 会“抖”一下？

利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。

### `order by` 是怎么工作的？

用 explain 命令查看执行计划时，Extra 这个字段中的“Using filesort”表示的就是需要排序。

#### 全字段排序

```sql
select city,name,age from t where city='杭州' order by name limit 1000;
```

这个语句执行流程如下所示 ：

1. 初始化 sort_buffer，确定放入 name、city、age 这三个字段；
2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；
3. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；
4. 从索引 city 取下一个记录的主键 id；
5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220728090300.png)

按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

外部排序一般使用归并排序算法。可以这么简单理解，**MySQL 将需要排序的数据分成 N 份，每一份单独排序后存在这些临时文件中。然后把这 N 个有序文件再合并成一个有序的大文件。**

#### rowid 排序

如果表的字段太多，导致单行太大，那么全字段排序的效率就不够好。

这种情况下，Mysql 可以采用 rowid 排序，相比于全字段排序，它的主要差异在于：

取行数据时，不取出整行，而只是取出 id 和用于排序的字段。当排序结束后，再根据 id 取出要查询的字段返回给客户端。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220728090919.png)

#### 全字段排序 VS rowid 排序

如果内存足够大，Mysql 会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

如果内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

并不是所有的 order by 语句，都需要排序操作的。MySQL 之所以需要生成临时表，并且在临时表上做排序操作，**其原因是原来的数据都是无序的。**如果能保证排序字段命中索引，那么就无需再排序了。

**覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。**

## 索引篇

### 索引的优点和缺点是什么？

✔️️️️️️️ 索引的优点：

- **索引大大减少了服务器需要扫描的数据量**，从而加快检索速度。
- **索引可以帮助服务器避免排序和临时表**。
- **索引可以将随机 I/O 变为顺序 I/O**。
- 支持行级锁的数据库，如 InnoDB 会在访问行的时候加锁。**使用索引可以减少访问的行数，从而减少锁的竞争，提高并发**。
- 唯一索引可以确保每一行数据的唯一性，通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能。

❌ 索引的缺点：

- **创建和维护索引要耗费时间**，这会随着数据量的增加而增加。
- **索引需要占用额外的物理空间**，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。
- **写操作（`INSERT`/`UPDATE`/`DELETE`）时很可能需要更新索引，导致数据库的写操作性能降低**。

基于以上，可以归纳出索引的基本使用规则：

- 索引不是越多越好，不要为所有列都创建索引
- 要尽量避免冗余和重复索引
- 要考虑删除未使用的索引
- 尽量的扩展索引，不要新建索引
- 频繁作为 WHERE 过滤条件的列应该考虑添加索引

### 何时适用索引？何时不适用索引？

✔️️️️️️️ 什么情况**适用**索引：

- **频繁读操作（ `SELECT` ）**
- **表的数据量比较大**。
- **列名经常出现在 `WHERE` 或连接（`JOIN`）条件中**。
- **列名经常用于 `GROUP BY` 或 `ORDER BY`**。

❌ 什么情况**不适用**索引：

- **频繁写操作**（ `INSERT`/`UPDATE`/`DELETE` ），也就意味着需要更新索引。
- **列名不经常出现在 `WHERE` 或连接（`JOIN`）条件中**，也就意味着索引会经常无法命中，没有意义，还增加空间开销。
- **非常小的表**，对于非常小的表，大部分情况下简单的全表扫描更高效。
- **特大型的表**，建立和使用索引的代价将随之增长。可以考虑使用分区技术或 Nosql。

### 索引有哪些常见数据结构？

Mysql 索引的常见数据结构：

- **hash 索引**
  - 因为索引数据结构紧凑，所以**查询速度非常快**。
  - **只支持等值比较查询** - 包括 `=`、`IN()`、`<=>`；**不支持任何范围查询**，如 `WHERE price > 100`。
  - **无法用于排序** - 因为哈希索引数据不是按照索引值顺序存储的。
  - **不支持部分索引匹配查找** - 因为哈希索引时使用索引列的全部内容来进行哈希计算的。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。
  - **不能用索引中的值来避免读取行** - 因为哈希索引只包含哈希值和行指针，不存储字段，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能影响不大。
  - 哈希索引有**可能出现哈希冲突**
    - 出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。
    - 如果哈希冲突多的话，维护索引的代价会很高。
- B+ 树索引
  - 适用于**全键值查找**、**键值范围查找**和**键前缀查找**，其中键前缀查找只适用于最左前缀查找。
  - 所有的关键字（可以理解为数据）都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。
  - 所有的叶子节点由指针连接。

下面是 Mysql 常用存储引擎对一些主要索引数据结构的支持：

| 索引数据结构/存储引擎 | InnoDB 引擎 | MyISAM 引擎 | Memory 引擎 |
| --------------------- | ----------- | ----------- | ----------- |
| **B+ 树索引**         | ✔️️️️️️️        | ✔️️️️️️️        | ✔️️️️️️️        |
| **Hash 索引**         | ❌          | ❌          | ✔️️️️️️️        |
| **Full Text 索引**    | ✔️️️️️️️        | ✔️️️️️️️        | ❌          |

### 什么是聚簇索引？什么是非聚簇索引？

根据叶子节点的内容，索引类型分为主键索引和非主键索引。

- 主键索引又被称为**『聚簇索引（clustered index）』，其叶子节点存的是整行数据**。
  - 聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。
  - 因为无法同时把数据行存放在两个不同的地方，所以**一个表只能有一个聚簇索引**。
  - InnoDB 的聚簇索引实际是在同一个结构中保存了 B 树的索引和数据行。
- 非主键索引又被称为**『二级索引（secondary index）』，其叶子节点存的是主键的值**。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。

### 聚簇索引和非聚簇索引的查询有什么区别

- 如果语句是 `select * from T where ID=500`，即聚簇索引查询方式，则只需要搜索主键索引树；
- 如果语句是 `select * from T where k=5`，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为**回表**。

也就是说，**基于非聚簇索引的查询需要多扫描一棵索引树**。因此，我们在应用中应该尽量使用主键查询。

**显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。**

### 为什么 InnoDB 选择 B+ 树作为索引的数据结构

- B+ 树 vs B 树
  - B+ 树只在叶子节点存储数据，而 B 树的非叶子节点也要存储数据，所以 B+ 树的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。
  - 另外，B+ 树叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。
- B+树 vs 二叉树
  - 对于有 N 个叶子节点的 B+ 树，其搜索复杂度为 `O(logdN)`，其中 d 表示节点允许的最大子节点个数为 d 个。
  - 在实际的应用当中， d 值是大于 100 的，这样就保证了，即使数据达到千万级别时，B+ 树的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。
  - 而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 `O(logN)`，这已经比 B+ 树高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。
- B+树 vs Hash
  - Hash 在做等值查询的时候效率贼快，搜索复杂度为 `O(1)`。
  - 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+ 树索引要比 Hash 表索引有着更广泛的适用场景的原因。

### 索引有哪些优化策略？

#### 索引基本原则

- **索引不是越多越好，不要为所有列都创建索引**。要考虑到索引的维护代价、空间占用和查询时回表的代价。索引一定是按需创建的，并且要尽可能确保足够轻量。一旦创建了多字段的联合索引，我们要考虑尽可能利用索引本身完成数据查询，减少回表的成本。
- 要**尽量避免冗余和重复索引**。
- 要**考虑删除未使用的索引**。
- **尽量的扩展索引，不要新建索引**。
- **频繁作为 `WHERE` 过滤条件的列应该考虑添加索引**。

#### 覆盖索引

**覆盖索引是指：索引上的信息足够满足查询请求，不需要回表查询数据**。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段**。

#### 最左前缀匹配原则

**这里的最左前缀，可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符**。

如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否**存在（相等）**，遇到范围查询 (`>`、`<`、`BETWEEN`、`LIKE`) 就**不能进一步匹配**了，后续退化为线性查找。因此，**列的排列顺序决定了可命中索引的列数**。

**应该将选择性高的列或基数大的列优先排在多列索引最前列**。**『索引的选择性』是指不重复的索引值和记录总数的比值**，选择性越高，查询效率越高。但有时，也需要考虑 `WHERE` 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。

#### 前缀索引

**『前缀索引』是指索引开始的部分字符**。对于 `BLOB`/`TEXT`/`VARCHAR` 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。

前缀索引的优点是可以**大大节约索引空间**，从而**提高索引效率**。

前缀索引的缺点是**会降低索引的区分度**。此外，**`order by` 无法使用前缀索引，无法把前缀索引用作覆盖索引**。

#### 独立索引

- 索引列不能是表达式的一部分，也不能是函数的参数
- 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

使用索引扫描来排序：ORDER BY 的字段作为索引，这样命中索引的查询结果，不需要额外排序

= 和 in 可以乱序：不需要考虑 =、IN 等的顺序，Mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。

### 哪些情况下，索引会失效？

导致索引失效的情况有：

- 对索引使用左或者左右模糊匹配 
- 对索引使用函数或表达式
- 对索引隐式类型转换
- 联合索引不遵循最左匹配原则
- WHERE 子句中的 OR

### 普通索引和唯一索引，应该怎么选择？

普通索引和唯一索引的**查询性能相差微乎其微**。

### 哪种 count 性能最好？

> 先说结论：按照效率排序的话，`count(字段)` < `count(主键 id)` < `count(1)` ≈ `count(*)`。**推荐采用 `count(*)`** 。

**对于 `count(主键 id)` 来说**，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。

**对于 `count(1)` 来说**，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

单看这两个用法的差别的话，你能对比出来，`count(1)` 执行得要比 `count(主键 id)` 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。

**对于 `count(字段)` 来说**：

1. 如果这个“字段”是定义为 `not null` 的话，一行行地从记录里面读出这个字段，判断不能为 `null`，按行累加；
2. 如果这个“字段”定义允许为 `null`，那么执行的时候，判断到有可能是 `null`，还要把值取出来再判断一下，不是 `null` 才累加。

也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。

**但是 `count(*)` 是例外**，并不会把全部字段取出来，而是专门做了优化，不取值。`count(*)` 肯定不是 `null`，按行累加。

## 事务篇

### 事务隔离级别

#### 事务隔离级别有哪些？分别解决了什么事务问题？

未提交读——丢失修改

已提交读——脏读

可重复读——不可重复读

串行读——幻读

#### Mysql 默认事务隔离是什么？

### 事务到底是隔离的还是不隔离的

#### “快照”在 MVCC 里是怎么工作的？

InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。

而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726083656.png)

图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。

图中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。

数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。

这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726085300.png)

这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
   a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
   b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

**InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726085703.png)

#### 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220726090537.png)

**事务的可重复读的能力是怎么实现的？**

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

### 幻读是什么，幻读有什么问题？

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。

#### 幻读是什么

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

说明：

- 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
- 幻读专指“新插入的行”。

#### 幻读有什么问题

- 语义上的破坏
- 数据一致性的问题
- 即使把所有记录都加上锁，还是阻止不了新插入的记录

#### 如何解决幻读

- 间隙锁（`gap lock`）
- `select * from t where d=5 for update`，不止给数据库已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新纪录
- next-key lock 是前开后闭区间
- 间隙锁和行锁合称为 `next-key lock`
- 跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作
- **间隙锁是在可重复读隔离级别下才会生效的**，所以把隔离级别设置为读提交的话，就没有间隙锁了。但同时，要解决可能出现的数据和日志不一致的问题，需要把`binlog`格式设置为`row`。

间隙锁和`next-key lock`的引入，帮我们解决了幻读问题，但同时也带来了一些困扰

- 间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响并发度的

## 锁篇

### Mysql 中有哪些锁？

为了解决并发访问题，Mysql 支持了很多种锁来实现不同程度的隔离性，以保证数据的安全性。

#### 独享锁和共享锁

InnoDB 实现标准行级锁定，根据是否独占资源，可以把锁分为两类：

- **独享锁（Exclusive）**，简写为 X 锁，又称**写锁**。
  - 独占锁允许持有该锁的事务更新或删除行。
  - 使用方式：`SELECT ... FOR UPDATE;`
- **共享锁（Shared）**，简写为 S 锁，又称**读锁**。
  - 共享锁允许持有该锁的事务读取一行。
  - 使用方式：`SELECT ... LOCK IN SHARE MODE;`

为什么要引入读写锁机制？

实际上，读写锁是一种通用的锁机制，并非 Mysql 的专利。在很多软件领域，都存在读写锁机制。

因为读操作本身是线程安全的，而一般业务往往又是读多写少的情况。因此，如果对读操作进行互斥，是不必要的，并且会大大降低并发访问效率。正式为了应对这种问题，产生了读写锁机制。

读写锁的特点是：**读读不互斥**、**读写互斥**、**写写互斥**。简言之：**只要存在写锁，其他事务就不能做任何操作**。

> 注：InnoDB 下的行锁、间隙锁、next-key 锁统统属于独享锁。

#### 悲观锁和乐观锁

基于加锁方式分类，Mysql 可以分为悲观锁和乐观锁。

- **悲观锁** - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作
  - 在查询完数据的时候就把事务锁起来，直到提交事务（`COMMIT`）
  - 实现方式：**使用数据库中的锁机制**。
- **乐观锁** - 假设最好的情况——每次访问数据时，都假设数据不会被其他线程修改，不必加锁。只在更新的时候，判断一下在此期间是否有其他线程更新该数据。
  - 实现方式：**更新数据时，先使用版本号机制或 CAS 算法检查数据是否被修改**。

为什么要引入乐观锁？

乐观锁也是一种通用的锁机制，在很多软件领域，都存在乐观锁机制。

**锁，意味着互斥，意味着阻塞。在高并发场景下，锁越多，阻塞越多，势必会拉低并发性能**。那么，为了提高并发度，能不能尽量不加锁呢？

乐观锁，顾名思义，就是假设最好的情况——每次访问数据时，都假设数据不会被其他线程修改，不必加锁。虽然不加锁，但不意味着什么都不做，而是在更新的时候，判断一下在此期间是否有其他线程更新该数据。乐观锁最常见的实现方式，是使用版本号机制或 CAS 算法（Compare And Swap）去实现。

乐观锁的**优点**是：减少锁竞争，提高并发度。

乐观锁的**缺点**是：

- **存在 ABA 问题**。所谓的 ABA 问题是指在并发编程中，如果一个变量初次读取的时候是 A 值，它的值被改成了 B，然后又其他线程把 B 值改成了 A，而另一个早期线程在对比值时会误以为此值没有发生改变，但其实已经发生变化了
- 如果乐观锁所检查的数据存在大量锁竞争，会由于**不断循环重试，产生大量的 CPU 开销**。

#### 全局锁、表级锁、行级锁

前文提到了，**锁，意味着互斥，意味着阻塞。在高并发场景下，锁越多，阻塞越多，势必会拉低并发性能**。在不得不加锁的情况下，显然，加锁的范围越小，锁竞争的发生频率就越小，系统的并发程度就越高。但是，加锁也需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销，**锁粒度越小，系统的锁操作开销就越大**。因此，在选择锁粒度时，也需要在锁开销和并发程度之间做一个权衡。

根据加锁的范围，MySQL 的锁大致可以分为：

- **全局锁** - 『全局锁』会锁定整个数据库。
- **表级锁（table lock）** - 『表级锁』锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。表级锁有：
  - **表锁**
  - **元数据锁（MDL）**
  - **意向锁（Intention Lock）**
  - **自增锁（AUTO-INC）**
- **行级锁（row lock）** - 『行级锁』锁定指定的行记录。这样其它线程还是可以对同一个表中的其它行记录进行操作。行级锁有：
  - **记录锁（Record Lock）**
  - **间隙锁（Gap Lock）**
  - **临键锁（Next-Key Lock）**
  - **插入意向锁**

以上各种加锁粒度，在不同存储引擎中的支持情况并不相同。如：InnoDB 支持全局锁、表级锁、行级锁；而 MyISAM 只支持全局锁、表级锁。

### 什么情况下会发生死锁？

**『死锁』是指两个或多个事务竞争同一资源，并请求锁定对方占用的资源，从而导致恶性循环的现象**。

产生死锁的场景：

- 当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。
- 多个事务同时锁定同一个资源时，也会产生死锁。

### 如何避免死锁？

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。由此可知，要想避免死锁，就要从这几个必要条件上去着手：

- 更新表时，**尽量使用主键更新**，减少冲突；
- **避免长事务**，尽量将长事务拆解，可以降低与其它事务发生冲突的概率；
- **设置合理的锁等待超时参数**，我们可以通过 `innodb_lock_wait_timeout` 设置合理的等待超时阈值，特别是在一些高并发的业务中，我们可以尽量将该值设置得小一些，避免大量事务等待，占用系统资源，造成严重的性能开销。
- 在编程中**尽量按照固定的顺序来处理数据库记录**，假设有两个更新操作，分别更新两条相同的记录，但更新顺序不一样，有可能导致死锁；
- 在允许幻读和不可重复读的情况下，尽量使用读已提交事务隔离级别，可以避免 Gap Lock 导致的死锁问题；
- 还可以使用其它的方式来代替数据库实现幂等性校验。例如，使用 Redis 以及 ZooKeeper 来实现，运行效率比数据库更佳。

### 如何解决死锁？

当出现死锁以后，有两种策略：

- **设置事务等待锁的超时时间**。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。
- **开启死锁检测**，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 `on`，表示开启这个逻辑。

在 InnoDB 中，`innodb_lock_wait_timeout` 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。但是，我直接把这个时间设置成一个很小的值，比如 1s，也是不可取的。当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 `innodb_deadlock_detect` 的默认值本身就是 on。为了解决死锁问题，不同数据库实现了各自的死锁检测和超时机制。InnoDB 的处理策略是：**将持有最少行级排它锁的事务进行回滚**。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。因此，死锁检测可能会耗费大量的 CPU。

## HA

## 优化

### count(\*)这么慢，我该怎么办？

不同的 MySQL 引擎中，count(\*) 有不同的实现方式。

- MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(\*) 的时候会直接返回这个数，效率很高；
- 而 InnoDB 引擎就麻烦了，它执行 count(\*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

**为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢**

因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220727084306.png)

InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(\*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。

- MyISAM 表虽然 count(\*) 很快，但是不支持事务；
- show table status 命令虽然返回很快，但是不准确；
- InnoDB 表直接 count(\*) 会遍历全表，虽然结果准确，但会导致性能问题。

### 保存计数

可以使用 Redis 保存计数，但存在丢失更新一集数据不一致问题。

可以使用数据库其他表保存计数，但要用事务进行控制，增/删数据时，同步改变计数。

### 不同的 count 用法

**对于 count(主键 id) 来说**，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。

**对于 count(1) 来说**，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

**对于 count(字段) 来说**：

- 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；
- 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。

**但是 count(\*) 是例外**，并不会把全部字段取出来，而是专门做了优化，不取值。count(\*) 肯定不是 null，按行累加。

所以结论是：按照效率排序的话，`count(字段)<count(主键 id)<count(1)≈count(*)`，所以我建议你，尽量使用 count(\*)。

### 为什么这些 SQL 语句逻辑相同，性能却差异巨大？

#### 函数操作会破坏索引有序性

**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**

示例：

```sql
CREATE TABLE `tradelog` (
  `id` int(11) NOT NULL,
  `tradeid` varchar(32) DEFAULT NULL,
  `operator` int(11) DEFAULT NULL,
  `t_modified` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `tradeid` (`tradeid`),
  KEY `t_modified` (`t_modified`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

select count(*) from tradelog where month(t_modified)=7;
```

由于在 t_modified 字段加了 month() 函数操作，导致了全索引扫描。为了能够用上索引的快速定位能力，我们就要把 SQL 语句改成基于字段本身的范围查询。

```sql
select count(*) from tradelog where
    -> (t_modified >= '2016-7-1' and t_modified<'2016-8-1') or
    -> (t_modified >= '2017-7-1' and t_modified<'2017-8-1') or
    -> (t_modified >= '2018-7-1' and t_modified<'2018-8-1');
```

#### 隐式转换

下面两个 SQL 的执行流程相同：

```sql
select * from tradelog where tradeid=110717;
select * from tradelog where CAST(tradid AS signed int) = 110717;
```

交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。这是由于这条语句隐式增加了转换函数，而对索引字段做函数操作，优化器会放弃走树搜索功能。

#### 隐式字符编码转换

示例：

```sql
CREATE TABLE `trade_detail` (
  `id` int(11) NOT NULL,
  `tradeid` varchar(32) DEFAULT NULL,
  `trade_step` int(11) DEFAULT NULL, /* 操作步骤 */
  `step_info` varchar(32) DEFAULT NULL, /* 步骤信息 */
  PRIMARY KEY (`id`),
  KEY `tradeid` (`tradeid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

insert into tradelog values(1, 'aaaaaaaa', 1000, now());
insert into tradelog values(2, 'aaaaaaab', 1000, now());
insert into tradelog values(3, 'aaaaaaac', 1000, now());

insert into trade_detail values(1, 'aaaaaaaa', 1, 'add');
insert into trade_detail values(2, 'aaaaaaaa', 2, 'update');
insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit');
insert into trade_detail values(4, 'aaaaaaab', 1, 'add');
insert into trade_detail values(5, 'aaaaaaab', 2, 'update');
insert into trade_detail values(6, 'aaaaaaab', 3, 'update again');
insert into trade_detail values(7, 'aaaaaaab', 4, 'commit');
insert into trade_detail values(8, 'aaaaaaac', 1, 'add');
insert into trade_detail values(9, 'aaaaaaac', 2, 'update');
insert into trade_detail values(10, 'aaaaaaac', 3, 'update again');
insert into trade_detail values(11, 'aaaaaaac', 4, 'commit');

SELECT d.*
FROM tradelog l, trade_detail d
WHERE d.tradeid = l.tradeid AND l.id = 2;
# 等价于
select * from trade_detail  where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value;

# 不需要做字符编码转换
EXPLAIN
SELECT l.operator
FROM tradelog l, trade_detail d
WHERE d.tradeid = l.tradeid AND d.id = 2;
```

字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。

### 为什么我只查一行的语句，也执行这么慢？

#### 查询长时间不返回

查询结果长时间不返回。

一般碰到这种情况的话，大概率是表被锁住了。接下来分析原因的时候，一般都是首先执行一下 show processlist 命令，看看当前语句处于什么状态。

使用 show processlist 命令查看 Waiting for table metadata lock 的示意图

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220801160916.png)

出现**这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。**

### MySQL 有哪些“饮鸩止渴”提高性能的方法？

#### 短连接风暴

短连接模式就是连接到数据库后，执行很少的 SQL 语句就断开，下次需要的时候再重连。

- MySQL 建立连接的成本很高。
  - 除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。
- 短连接模型存在一个风险：一旦数据库处理速度很慢，连接数就会暴涨。
- **`max_connections` 控制一个 MySQL 实例同时存在的连接数的上限**。
  - 超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。

#### 解决方法 1：先处理掉那些占着连接但是不工作的线程

- `show processlist` 查看 `sleep` 的线程，然后干掉空闲的连接。注意：可能会误杀事务。
- 应该优先断开事务外空闲的连接。
  - 通过查 `information_schema` 库的 `innodb_trx` 表判断是否处于事务中。
- 再考虑断开事务内空闲太久的连接。

#### 解决方法 2：减少连接过程的消耗

如果想短时间创建大量数据库连接，有一种做法是跳过权限验证。

跳过权限验证的方法是：重启数据库，并使用 `–skip-grant-tables` 参数启动。

注意：此方法风险极高，不建议使用。

### 慢查询性能问题

一般有三种可能：

1. 索引没有设计好；
2. SQL 语句没写好；
3. MySQL 选错了索引。
   - 可以通过 `force index` 强制使用某个索引

### QPS 突增问题

有时候由于业务突然出现高峰，或者应用程序 bug，导致某个语句的 QPS 突然暴涨，也可能导致 MySQL 压力过大，影响服务。

应对方法：

1. 一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。
2. 如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0。
3. 如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这时，我们可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成"select 1"返回。
   - 这个方法是用于止血的，但风险很高，不建议使用。

个人观点：以上方法都是基于 DBA 视角的处理方式。实际环境中，应该做好数据库 QPS、CPU 监控，如果发现请求量激增，快要达到瓶颈，可以先紧急弹性扩容，保障业务不损失。然后排查原因，是否是新业务设计不当导致、是否是大数据在也业务高峰期进行数据分析导致，等等。

### join 语句如何优化

**大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。**

#### MRR

MRR 优化后的语句执行流程：

1. 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ;
2. 将 read_rnd_buffer 中的 id 进行递增排序；
3. 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。

这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。

**MRR 能够提升性能的核心**在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。临时表在使用上有以下几个特点：

1. 建表语法是 create temporary table …。
2. 一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。
3. 临时表可以与普通表同名。
4. session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。
5. show tables 命令不显示临时表。

#### 到底可不可以使用 join

1. 如果可以使用被驱动表的索引，join 语句还是有其优势的；
2. 不能使用被驱动表的索引，只能使用 Block Nested-Loop Join 算法，这样的语句就尽量不要使用；
3. 在使用 join 的时候，应该让小表做驱动表。

### 我查了这么多数据会不会把数据库内存打爆

#### 全表扫描对 server 层的影响

**MySQL 是“边读边发的”**

InnoDB 的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表 t 的主键索引。这条查询语句由于没有其他的判断条件，所以查到的每一行都可以直接放到结果集里面，然后返回给客户端。

查询的结果是分段发给客户端的，因此扫描全表，查询返回大量的数据，并不会把内存打爆。

#### 全表扫描对 InnoDB 的影响

对于 InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。

## 存储引擎

### 都说 InnoDB 好，那还要不要使用 Memory 引擎

InnoDB 和 Memory 引擎的数据组织方式是不同的：

- InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，我们称之为**索引组织表**（Index Organizied Table）。
- 而 Memory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为**堆组织表**（Heap Organizied Table）。

内存表不支持行锁，只支持表锁。

数据放在内存中，是内存表的优势，但也是一个劣势。因为，数据库重启的时候，所有的内存表都会被清空。

## 其他

### 为什么临时表可以重名

临时表在使用上有以下几个特点：

1. 建表语法是 create temporary table …。
2. 一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。
3. 临时表可以与普通表同名。
4. session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。
5. show tables 命令不显示临时表。

**临时表特别适合 join 优化这种场景**，原因是：

1. 不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。
2. 不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。

#### 临时表的应用

由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。其中，分库分表系统的跨库查询就是一个典型的使用场景。

分库分表两种实现思路：

**第一种思路是，**在 proxy 层的进程代码中实现排序。

这种方式的优势是处理速度快，拿到分库的数据以后，直接在内存中参与计算。不过，这个方案的缺点也比较明显：

1. 需要的开发工作量比较大。我们举例的这条语句还算是比较简单的，如果涉及到复杂的操作，比如 group by，甚至 join 这样的操作，对中间层的开发能力要求比较高；
2. 对 proxy 端的压力比较大，尤其是很容易出现内存不够用和 CPU 瓶颈的问题。

**另一种思路就是，**把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中，然后在这个汇总实例上做逻辑操作。

比如上面这条语句，执行流程可以类似这样：

- 在汇总库上创建一个临时表 temp_ht，表里包含三个字段 v、k、t_modified；
- 在各个分库上执行

```sql
select v,k,t_modified from ht_x where k >= M order by t_modified desc limit 100;
```

- 把分库执行的结果插入到 temp_ht 表中；
- 执行

```
select v from temp_ht order by t_modified desc limit 100;
```

得到结果。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/20220828152038.png)

**在实践中，我们往往会发现每个分库的计算量都不饱和，所以会直接把临时表 temp_ht 放到 32 个分库中的某一个上。**

#### 什么时候会使用内部临时表

1. 如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null；
2. 尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort；
3. 如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表；
4. 如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果。

### 自增主键为什么不是连续的

**表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。**

在 MyISAM 引擎里面，自增值是被写在数据文件上的。而在 InnoDB 中，自增值是被记录在内存的。

InnoDB 中，只保证了自增 id 是递增的，但不保证是连续的。这么做是处于性能考虑：语句执行失败也不回退自增 id。

### grant 之后为什么要跟着 flush privilege

grant 语句会同时修改数据表和内存，判断权限的时候使用的是内存数据。因此，规范地使用 grant 和 revoke 语句，是不需要随后加上 flush privileges 语句的。

flush privileges 语句本身会用数据表的数据重建一份内存权限数据，所以在权限数据可能存在不一致的情况下再使用。而这种不一致往往是由于直接用 DML 语句操作系统权限表导致的，所以我们尽量不要使用这类语句。

## 参考资料

- [《高性能 MySQL》](https://book.douban.com/subject/23008813/)
- [MySQL 实战 45 讲](https://time.geekbang.org/column/intro/139)
- [图解 MySQL 介绍](https://xiaolincoding.com/mysql/)